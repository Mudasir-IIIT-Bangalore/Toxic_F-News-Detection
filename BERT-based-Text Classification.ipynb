{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d714c3",
   "metadata": {},
   "source": [
    "Transfer Learning for NLP: Fine-Tuning BERT for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffec3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d6bbab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.89E+17</td>\n",
       "      <td>The lack of this understanding is a small but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.89E+17</td>\n",
       "      <td>i just told my parents about my depression and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.89E+17</td>\n",
       "      <td>depression is something i don't speak about ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.89E+17</td>\n",
       "      <td>Made myself a tortilla filled with pb&amp;j. My de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.89E+17</td>\n",
       "      <td>@WorldofOutlaws I am gonna need depression med...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                               text  label\n",
       "0  9.89E+17  The lack of this understanding is a small but ...      1\n",
       "1  9.89E+17  i just told my parents about my depression and...      1\n",
       "2  9.89E+17  depression is something i don't speak about ev...      1\n",
       "3  9.89E+17  Made myself a tortilla filled with pb&j. My de...      1\n",
       "4  9.89E+17  @WorldofOutlaws I am gonna need depression med...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Depressed(tweets_posts_news_others).csv\", encoding = 'unicode_escape', engine ='python')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53d1d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc24a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aee93c61d354078b9c044bca526e2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf2974d80974c338369878362228427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16b86d51f954013a55e617ad4dca574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bb9a528e334dc6bc77a474586fc5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4976b2e0ff4c4171bd2fdd4c249984c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb67afb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0, 0, 0, 0], [101, 2057, 2097, 3231, 2182, 1998, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "text = [\"this is a bert model tutorial\", \"we will test here and fine-tune a bert model\"]\n",
    "\n",
    "# encode text\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding=True)\n",
    "\n",
    "# output\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd0f1d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "\n",
    "seq_len = [len(str(i).split()) for i in train_text] \n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e0d9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 19,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 19,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 19,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a9803cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the integer sequences to tensors.\n",
    "\n",
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79d30459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bc47c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8266cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        #_, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b03f1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "adcca91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93bb26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89ae936d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.utils.class_weight import compute_class_weight\\n\\n#compute the class weights\\nclass_weights = compute_class_weight(\\'balanced\\', np.unique(train_labels), train_labels)\\n\\nprint(\"Class Weights:\",class_weights)\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec319a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [2.15171192 0.65135811]\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labels),\n",
    "                                        y = train_labels                                                    \n",
    "                                    )\n",
    "#class_weights = dict(zip(np.unique(train_labels), class_weights))\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1fe2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f359c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1bfca276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef5b2a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.687\n",
      "Validation Loss: 0.674\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.663\n",
      "Validation Loss: 0.651\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.641\n",
      "Validation Loss: 0.629\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.624\n",
      "Validation Loss: 0.609\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.601\n",
      "Validation Loss: 0.590\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.581\n",
      "Validation Loss: 0.568\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.563\n",
      "Validation Loss: 0.551\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.543\n",
      "Validation Loss: 0.528\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.525\n",
      "Validation Loss: 0.510\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    114.\n",
      "  Batch   100  of    114.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.507\n",
      "Validation Loss: 0.494\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a7cbb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3ed0c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fca9e892",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.92      0.75       182\n",
      "           1       0.97      0.84      0.90       600\n",
      "\n",
      "    accuracy                           0.86       782\n",
      "   macro avg       0.80      0.88      0.83       782\n",
      "weighted avg       0.89      0.86      0.87       782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb3056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
